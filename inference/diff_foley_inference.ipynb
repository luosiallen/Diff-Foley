{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff-Foley: Inference Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/\".join(os.getcwd().split(\"/\")[:-1]))\n",
    "from diff_foley.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Stage1 CAVP Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "import os\n",
    "from demo_util import Extract_CAVP_Features \n",
    "\n",
    "# Set Device:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\")\n",
    "# Default Setting:\n",
    "\n",
    "fps = 4                                                     #  CAVP default FPS=4, Don't change it.\n",
    "batch_size = 8    \n",
    "cavp_config_path = \"./config/Stage1_CAVP.yaml\"              #  CAVP Config\n",
    "cavp_ckpt_path = \"./diff_foley_ckpt/cavp_epoch66.ckpt\"      #  CAVP Ckpt\n",
    "\n",
    "\n",
    "# Initalize CAVP Model:\n",
    "extract_cavp = Extract_CAVP_Features(fps=fps, batch_size=batch_size, device=device, config_path=cavp_config_path, ckpt_path=cavp_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Stage2 LDM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "# LDM Config:\n",
    "ldm_config_path = \"./config/Stage2_LDM.yaml\"\n",
    "ldm_ckpt_path = \"./diff_foley_ckpt/ldm_epoch240.ckpt\"\n",
    "config = OmegaConf.load(ldm_config_path)\n",
    "\n",
    "# Loading LDM:\n",
    "latent_diffusion_model = load_model_from_config(config, ldm_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_path ./demo_videos/car.mp4\n",
      "truncate second:  8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 34 Total: 33.0: : 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample1:\n",
    "video_path = \"./demo_videos/gun.mp4\"\n",
    "save_path = \"./generate_samples/gun\"\n",
    "tmp_path = \"./generate_samples/temp_folder\" \n",
    "\n",
    "## Sample2:\n",
    "# video_path = \"./demo_videos/drum.mp4\"\n",
    "# save_path = \"./generate_samples/drum\"\n",
    "# tmp_path = \"./generate_samples/temp_folder\" \n",
    "\n",
    "## Sample3:\n",
    "# video_path = \"./demo_videos/car.mp4\"\n",
    "# save_path = \"./generate_samples/car\"\n",
    "# tmp_path = \"./generate_samples/temp_folder\" \n",
    "\n",
    "\n",
    "start_second = 0              # Video start second\n",
    "truncate_second = 8.2         # Video end = start_second + truncate_second\n",
    "\n",
    "# Extract Video CAVP Features & New Video Path:\n",
    "cavp_feats, new_video_path = extract_cavp(video_path, start_second, truncate_second, tmp_path=tmp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Diff-Foley Generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.(a) Double Guidance Load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "# Whether use Double Guidance:\n",
    "use_double_guidance = True\n",
    "\n",
    "if use_double_guidance:\n",
    "    classifier_config_path = \"./config/Double_Guidance_Classifier.yaml\"\n",
    "    classifier_ckpt_path = \"./diff_foley_ckpt/double_guidance_classifier.ckpt\"\n",
    "    classifier_config = OmegaConf.load(classifier_config_path)\n",
    "    classifier = load_model_from_config(classifier_config, classifier_ckpt_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 33, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Window::   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Double Guidance: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current samples:: 100%|██████████| 4/4 [00:24<00:00,  6.10s/it]\n",
      "Window:: 100%|██████████| 1/1 [00:29<00:00, 29.98s/it]\n"
     ]
    }
   ],
   "source": [
    "from demo_util import inverse_op\n",
    "\n",
    "sample_num = 4\n",
    "\n",
    "# Inference Param:\n",
    "cfg_scale = 4.5      # Classifier-Free Guidance Scale\n",
    "cg_scale = 50        # Classifier Guidance Scale\n",
    "\n",
    "\n",
    "steps = 25                # Inference Steps\n",
    "\n",
    "sampler = \"DPM_Solver\"    # DPM-Solver Sampler\n",
    "# sampler = \"DDIM\"        # DDIM Sampler\n",
    "# sampler = \"PLMS\"        # PLMS Sampler\n",
    "\n",
    "\n",
    "save_path = save_path + \"_CFG{}_CG{}_{}_{}_useDG_{}\".format(cfg_scale, cg_scale, sampler, steps, use_double_guidance)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Video CAVP Features:\n",
    "video_feat = torch.from_numpy(cavp_feats).unsqueeze(0).repeat(sample_num, 1, 1).to(device)\n",
    "print(video_feat.shape)\n",
    "\n",
    "\n",
    "# Truncate the Video Cond:\n",
    "feat_len = video_feat.shape[1]\n",
    "truncate_len = 32\n",
    "window_num = feat_len // truncate_len\n",
    "\n",
    "\n",
    "audio_list = []     # [sample_list1, sample_list2, sample_list3 ....]\n",
    "for i in tqdm(range(window_num), desc=\"Window:\"):\n",
    "    start, end = i * truncate_len, (i+1) * truncate_len\n",
    "    \n",
    "    # 1). Get Video Condition Embed:\n",
    "    embed_cond_feat = latent_diffusion_model.get_learned_conditioning(video_feat[:, start:end])     \n",
    "\n",
    "    # 2). CFG unconditional Embedding:\n",
    "    uncond_cond = torch.zeros(embed_cond_feat.shape).to(device)\n",
    "    \n",
    "    # 3). Diffusion Sampling:\n",
    "    print(\"Using Double Guidance: {}\".format(use_double_guidance))\n",
    "    if use_double_guidance:\n",
    "        audio_samples, _ = latent_diffusion_model.sample_log_with_classifier_diff_sampler(embed_cond_feat, origin_cond=video_feat, batch_size=video_feat.shape[0], sampler_name=sampler, ddim_steps=steps, unconditional_guidance_scale=cfg_scale,unconditional_conditioning=uncond_cond,classifier=classifier, classifier_guide_scale=cg_scale)  # Double Guidance\n",
    "    else:\n",
    "        audio_samples, _ = latent_diffusion_model.sample_log_diff_sampler(embed_cond_feat, batch_size=sample_num, sampler_name=sampler, ddim_steps=steps, unconditional_guidance_scale=cfg_scale,unconditional_conditioning=uncond_cond)           #  Classifier-Free Guidance\n",
    " \n",
    "    # 4). Decode Latent:\n",
    "    audio_samples = latent_diffusion_model.decode_first_stage(audio_samples)                     \n",
    "    audio_samples = audio_samples[:, 0, :, :].detach().cpu().numpy()                               \n",
    "\n",
    "    # 5). Spectrogram -> Audio:  (Griffin-Lim Algorithm)\n",
    "    sample_list = []        #    [sample1, sample2, ....]\n",
    "    for k in tqdm(range(audio_samples.shape[0]), desc=\"current samples:\"):\n",
    "        sample = inverse_op(audio_samples[k])\n",
    "        sample_list.append(sample)\n",
    "    audio_list.append(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130816,)\n",
      "(130816,)\n",
      "(130816,)\n",
      "(130816,)\n",
      "Gen Success !!\n"
     ]
    }
   ],
   "source": [
    "# Save Samples:\n",
    "path_list = []\n",
    "for i in range(sample_num):      # sample_num\n",
    "    current_audio_list = []\n",
    "    for k in range(window_num):\n",
    "        current_audio_list.append(audio_list[k][i])\n",
    "    current_audio = np.concatenate(current_audio_list,0)\n",
    "    print(current_audio.shape)\n",
    "    sf.write(os.path.join(save_path, \"sample_{}_diff.wav\").format(i), current_audio, 16000)\n",
    "    path_list.append(os.path.join(save_path, \"sample_{}_diff.wav\").format(i))\n",
    "print(\"Gen Success !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Success !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "# Concat The Video and Sound:\n",
    "import subprocess\n",
    "src_video_path = new_video_path\n",
    "for i in range(sample_num):\n",
    "    gen_audio_path = path_list[i]\n",
    "    out_path = os.path.join(save_path, \"output_{}.mp4\".format(i))\n",
    "    # cmd = [\"ffmpeg\" ,\"-i\" ,src_video_path,\"-i\" , gen_audio_path ,\"-c:v\" ,\"copy\" ,\"-c:a\" ,\"aac\" ,\"-strict\" ,\"experimental\", out_path]\n",
    "    cmd = [\"ffmpeg\" ,\"-i\" ,src_video_path,\"-i\" , gen_audio_path ,\"-c:v\" ,\"copy\" ,\"-c:a\" ,\"mp3\" ,\"-strict\" ,\"experimental\", out_path]\n",
    "    subprocess.check_call(cmd)\n",
    "print(\"Gen Success !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specvqgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
